Going for ./pytorch_data/balanced_hcp_4split_gender_fmri_50_5_roi_norm_False
Size is: 3001 / 751
Positive classes: 1500.0 / 376.0
For  {'lr': 0.001, 'weight_decay': 0}
Number of trainable params: 1833050
2-0-Epoch: 0, Loss: 306.35906 / 306.38316
2-0-Epoch: 1, Loss: 291.58514 / 291.53195
2-0-Epoch: 2, Loss: 277.99456 / 277.85942
2-0-Epoch: 3, Loss: 266.76094 / 266.50511
2-0-Epoch: 4, Loss: 257.79527 / 257.42544
2-0-Epoch: 5, Loss: 250.31197 / 249.8394
2-0-Epoch: 6, Loss: 244.42521 / 243.89131
2-0-Epoch: 7, Loss: 239.33931 / 238.72962
2-0-Epoch: 8, Loss: 235.13216 / 234.48704
2-0-Epoch: 9, Loss: 231.25021 / 230.56456
2-0-Epoch: 10, Loss: 228.00075 / 227.28774
2-0-Epoch: 11, Loss: 225.20555 / 224.46882
2-0-Epoch: 12, Loss: 223.20294 / 222.42604
2-0-Epoch: 13, Loss: 221.36965 / 220.57399
2-0-Epoch: 14, Loss: 220.00725 / 219.19255
2-0-Epoch: 15, Loss: 218.67707 / 217.84596
2-0-Epoch: 16, Loss: 217.71062 / 216.87328
2-0-Epoch: 17, Loss: 216.97034 / 216.11745
2-0-Epoch: 18, Loss: 216.13367 / 215.26732
2-0-Epoch: 19, Loss: 215.60554 / 214.72963
2-0-Epoch: 20, Loss: 215.06166 / 214.18123
2-0-Epoch: 21, Loss: 214.63178 / 213.74759
2-0-Epoch: 22, Loss: 214.22543 / 213.33621
2-0-Epoch: 23, Loss: 213.86572 / 212.96735
2-0-Epoch: 24, Loss: 213.54462 / 212.63757
2-0-Epoch: 25, Loss: 213.26117 / 212.34509
2-0-Epoch: 26, Loss: 213.00553 / 212.07754
2-0-Epoch: 27, Loss: 212.7881 / 211.84982
2-0-Epoch: 28, Loss: 212.58947 / 211.63864
2-0-Epoch: 29, Loss: 212.42791 / 211.46596
2-0-Epoch: 30, Loss: 212.2904 / 211.31557
2-0-Epoch: 31, Loss: 212.18062 / 211.19399
2-0-Epoch: 32, Loss: 212.09743 / 211.0977
2-0-Epoch: 33, Loss: 212.0393 / 211.02978
2-0-Epoch: 34, Loss: 212.01018 / 210.98917
2-0-Epoch: 35, Loss: 212.02585 / 210.99641
2-0-Epoch: 36, Loss: 212.12432 / 211.08749
2-0-Epoch: 37, Loss: 212.34102 / 211.29516
2-0-Epoch: 38, Loss: 212.72155 / 211.67607
2-0-Epoch: 39, Loss: 213.10131 / 212.0476
2-0-Epoch: 40, Loss: 213.03765 / 211.98732
2-0-Epoch: 41, Loss: 212.75543 / 211.69869
2-0-Epoch: 42, Loss: 212.74305 / 211.68619
2-0-Epoch: 43, Loss: 212.58982 / 211.52915
2-0-Epoch: 44, Loss: 212.6136 / 211.55053
2-0-Epoch: 45, Loss: 212.67459 / 211.60943
2-0-Epoch: 46, Loss: 212.84257 / 211.77148
2-0-Epoch: 47, Loss: 212.69847 / 211.63201
2-0-Epoch: 48, Loss: 212.67116 / 211.59749
2-0-Epoch: 49, Loss: 212.57385 / 211.49617
Best params:  logs/3layerAE_1.0_1200_2_0_0.001.pth ( 210.98916805324458 )
2--Final Loss: 212.23521
