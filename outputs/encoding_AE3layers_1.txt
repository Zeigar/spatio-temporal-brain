Going for ./pytorch_data/balanced_hcp_4split_gender_fmri_50_5_roi_norm_False
Size is: 2999 / 753
Positive classes: 1500.0 / 376.0
For  {'lr': 0.001, 'weight_decay': 0}
Number of trainable params: 1833050
1-0-Epoch: 0, Loss: 306.29788 / 306.14033
1-0-Epoch: 1, Loss: 291.51866 / 291.70093
1-0-Epoch: 2, Loss: 277.9699 / 278.53017
1-0-Epoch: 3, Loss: 266.61576 / 267.4733
1-0-Epoch: 4, Loss: 257.54193 / 258.64958
1-0-Epoch: 5, Loss: 250.07553 / 251.3781
1-0-Epoch: 6, Loss: 244.05654 / 245.48088
1-0-Epoch: 7, Loss: 239.07148 / 240.55103
1-0-Epoch: 8, Loss: 234.8676 / 236.39943
1-0-Epoch: 9, Loss: 230.92402 / 232.5074
1-0-Epoch: 10, Loss: 227.61798 / 229.23683
1-0-Epoch: 11, Loss: 224.97935 / 226.65493
1-0-Epoch: 12, Loss: 222.78021 / 224.50763
1-0-Epoch: 13, Loss: 220.98657 / 222.75685
1-0-Epoch: 14, Loss: 219.67722 / 221.46667
1-0-Epoch: 15, Loss: 218.25108 / 220.0535
1-0-Epoch: 16, Loss: 217.26679 / 219.08033
1-0-Epoch: 17, Loss: 216.54778 / 218.37898
1-0-Epoch: 18, Loss: 215.67691 / 217.5287
1-0-Epoch: 19, Loss: 215.14672 / 217.01262
1-0-Epoch: 20, Loss: 214.57802 / 216.45375
1-0-Epoch: 21, Loss: 214.13374 / 216.00743
1-0-Epoch: 22, Loss: 213.71827 / 215.60123
1-0-Epoch: 23, Loss: 213.3484 / 215.24667
1-0-Epoch: 24, Loss: 213.0204 / 214.9318
1-0-Epoch: 25, Loss: 212.72768 / 214.64657
1-0-Epoch: 26, Loss: 212.46898 / 214.3956
1-0-Epoch: 27, Loss: 212.24348 / 214.178
1-0-Epoch: 28, Loss: 212.04106 / 213.97962
1-0-Epoch: 29, Loss: 211.87471 / 213.81573
1-0-Epoch: 30, Loss: 211.73094 / 213.6708
1-0-Epoch: 31, Loss: 211.61644 / 213.55733
1-0-Epoch: 32, Loss: 211.52551 / 213.46073
1-0-Epoch: 33, Loss: 211.4576 / 213.39408
1-0-Epoch: 34, Loss: 211.41659 / 213.34863
1-0-Epoch: 35, Loss: 211.41646 / 213.3459
1-0-Epoch: 36, Loss: 211.47597 / 213.39913
1-0-Epoch: 37, Loss: 211.62714 / 213.5485
1-0-Epoch: 38, Loss: 211.94521 / 213.85773
1-0-Epoch: 39, Loss: 212.40178 / 214.30798
1-0-Epoch: 40, Loss: 212.62738 / 214.53092
1-0-Epoch: 41, Loss: 212.31186 / 214.21675
1-0-Epoch: 42, Loss: 212.14352 / 214.04513
1-0-Epoch: 43, Loss: 212.13847 / 214.04257
1-0-Epoch: 44, Loss: 212.04794 / 213.94697
1-0-Epoch: 45, Loss: 212.04559 / 213.94663
1-0-Epoch: 46, Loss: 212.08954 / 213.99267
1-0-Epoch: 47, Loss: 212.26315 / 214.15243
1-0-Epoch: 48, Loss: 212.23102 / 214.1216
1-0-Epoch: 49, Loss: 212.17408 / 214.06463
Best params:  logs/3layerAE_1.0_1200_1_0_0.001.pth ( 213.3459 )
1--Final Loss: 211.84361
